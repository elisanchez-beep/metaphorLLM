# metaphorLLM

This repository includes the data and code used in the paper **Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding.**

Our paper presents a comprehensive evaluation of the capabilities of Large Language Models (LLMs) in metaphor interpretation across multiple datasets, tasks, and prompt configurations. The results indicate that LLMs' performance is more influenced by features like lexical overlap and sentence length than by metaphorical content, demonstrating that any alleged emergent abilities of LLMs to understand metaphorical language are the result of a combination of surface-level features, in-context learning, and linguistic knowledge. This work provides critical insights into the current capabilities and limitations of LLMs in processing figurative language, highlighting the need for more realistic evaluation frameworks in metaphor interpretation tasks.

## DATA
For our evaluation, we used the following public datasets, gathered in `interpretation/`:
- Fig-QA: [Dataset](https://github.com/nightingal3/fig-qa)
- FLUTE: [Dataset](https://github.com/tuhinjubcse/model-in-the-loop-fig-lang)
- IMPLI: [Dataset](https://github.com/UKPLab/acl2022-impli)
- Figurative-NLI: [Dataset](https://github.com/tuhinjubcse/Figurative-NLI)
- Meta4XNLI: [Dataset](https://huggingface.co/datasets/HiTZ/meta4xnli)

In `/paraphrases/`:
`*.tsv` files include paraphrases generated by `Commandr-R+` and `Mistral-7B-Instruct`.with premise, hypothesis gold label and the metaphorical paraphrased_sentence (can be either premise or hypothesis).
The structure is organized in the same manner as the `interpretation/` folder. 


# PARAPHRASE_GEN

Code to generate literal paraphrases:
- `commandr.py`: code to generate paraphrases with Comman-R+ through Cohere's API
- `generate-paraphrases.py`: code to generate paraphrases with Mistral-7B-Instruct. Another model can  be used by specifying it as argument in `-model` in `.sh` files.
- `generate_scripts_paraphrase.py`: code to automatically generate `.sh` executables for datasets and models (they must be included in datasets and models variables).
- `Ã¬nstruct_p.sh`: example executable file to launch paraphrase generation. Arguments to load information from `config.json`.
  
# ZERO-SHOT
Code for evaluation in zero-, few-shot and Chain-of-thought setups.
- `zero_shot.py`: code to execute evaluation.
- `generate_scripts_zero_shot`: code to automatically generate `.sh` executables for evaluation with original datasets and literal paraphrases. Paths should be adapted.
- `chain.sh`: example to evaluate original metaphorical dataset with the specified model in `-model`argument. Predictions will be dumped in the path specified in `-output`.
  - `-prompt-type`: nli-zero, qa-few-ent, chain.
- `qa-few-ent_commandr.sh`: example to evaluate literal paraphrases generated with Commandr.
  - To evaluate paraphrases, `-paraphrases` flag must be added. Also, the model used to generate the paraphrases in `-paraphrases_source` (commandr, mistralinstruct). 
